# Gradient Descent and Stochastic Gradient Descent

This repository contains a Jupyter notebook (`gd_and_sgd.ipynb`) that demonstrates and explains the fundamentals of optimization algorithms used in machine learning ‚Äî specifically **Gradient Descent (GD)** and **Stochastic Gradient Descent (SGD)**.

## üß† Overview

The notebook includes:

- Explanation of the optimization problem
- Implementation of Gradient Descent (GD)
- Implementation of Stochastic Gradient Descent (SGD)
- Visualization of optimization trajectories
- Comparison between GD and SGD in terms of convergence and performance

## üìä Requirements

To run this notebook, ensure you have the following Python libraries installed:

```bash
pip install numpy matplotlib
```

## ‚ñ∂Ô∏è Getting Started

1. Clone the repository:
    ```bash
    git clone https://github.com/yourusername/gradient-descent-demo.git
    cd gradient-descent-demo
    ```

2. Launch the Jupyter Notebook:
    ```bash
    jupyter notebook gd_and_sgd.ipynb
    ```

## üìà Visuals

The notebook provides plots to visualize:
- Loss surface
- Update steps for GD and SGD
- Convergence behavior over iterations

## ü§ù Contributing

Feel free to fork the repo and submit pull requests to improve the explanations, add additional optimization techniques, or suggest improvements.
